{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow script mode training and serving\n",
    "\n",
    "This makes use of nima and transfer learning from mobilenet to make image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert AVA dataset into TFRecord\n",
    "\n",
    "Convert AVA dataset into TFRecord\n",
    "\n",
    "Here is the entire script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Script for converting AVA dataset.\"\"\"\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# The directory where the AVA dataset is stored\n",
    "ava_dir = '/home/ec2-user/ava_dataset'\n",
    "\n",
    "# The directory where the TFRecord files will be stored\n",
    "dataset_dir = '/home/ec2-user/ava_dataset/tfrecords'\n",
    "\n",
    "# Number of examples per TFRecord shard\n",
    "shard_num = 10000\n",
    "\n",
    "# Fraction of dataset for validation\n",
    "validation_split = 0.2\n",
    "\n",
    "\n",
    "with open(os.path.join(ava_dir, 'AVA.txt'), 'r') as f:\n",
    "    ava = [line.strip().split() for line in f.readlines()]\n",
    "\n",
    "image_path = tf.placeholder(dtype=tf.string)\n",
    "jpeg = tf.read_file(image_path)\n",
    "decoded = tf.image.decode_jpeg(jpeg, channels=3)\n",
    "\n",
    "counts = {'train': 0, 'validation': 0}\n",
    "writers = {}\n",
    "\n",
    "#os.makedirs(dataset_dir)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for item in ava:\n",
    "        filename = os.path.join(ava_dir, 'photos', item[1]) + '.jpg'\n",
    "        try:\n",
    "            image_data, _ = sess.run(\n",
    "                [jpeg, decoded], feed_dict={image_path: filename})\n",
    "\n",
    "            if random.random() > validation_split:\n",
    "                split = 'train'\n",
    "            else:\n",
    "                split = 'validation'\n",
    "\n",
    "            if split not in writers or counts[split] % shard_num == 0:\n",
    "                writer_path = os.path.join(\n",
    "                    dataset_dir, '{}_{}-{}.tfrecord'.format(\n",
    "                        split, counts[split],\n",
    "                        counts[split] + shard_num - 1))\n",
    "                writers[split] = tf.python_io.TFRecordWriter(writer_path)\n",
    "\n",
    "            scores = tf.train.FloatList(value=list(map(int, item[2:12])))\n",
    "            image = tf.train.BytesList(value=[image_data])\n",
    "            features = tf.train.Features(feature={\n",
    "                'scores': tf.train.Feature(float_list=scores),\n",
    "                'image': tf.train.Feature(bytes_list=image)})\n",
    "            example = tf.train.Example(features=features)\n",
    "            writers[split].write(example.SerializeToString())\n",
    "            counts[split] += 1\n",
    "        except:\n",
    "            print('Error decoding image: {}'.format(filename))\n",
    "\n",
    "for split, count in counts.items():\n",
    "    filename = '{}.txt'.format(split)\n",
    "    with open(os.path.join(dataset_dir, filename), 'w') as f:\n",
    "        f.write('{}\\n'.format(count))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for distributed training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "estimator = TensorFlow(base_job_name='cifar10',\n",
    "                       entry_point='cifar10_keras_sm.py',\n",
    "                       source_dir='nima',\n",
    "                       role=role,\n",
    "                       framework_version='1.12.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters={'epochs' : 1},\n",
    "                       train_instance_count=1, train_instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train' :  's3://waynetoh-ml/ava_dataset/train',\n",
    "             'validation' :  's3://waynetoh-ml/ava_dataset/train',\n",
    "             'eval' :  's3://waynetoh-ml/ava_dataset/train'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deploy the trained model to an endpoint\n",
    "\n",
    "The `deploy()` method creates a SageMaker model, which is then deployed to an endpoint to serve prediction requests in real time. We will use the TensorFlow Serving container for the endpoint, because we trained with script mode. This serving container runs an implementation of a web server that is compatible with SageMaker hosting protocol. The [Using your own inference code]() document explains how SageMaker runs inference containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the endpoint\n",
    "\n",
    "Let's download the training data and use that as input for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the endpoint\n",
    "\n",
    "Let's delete the endpoint we just created to prevent incurring any extra costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
